{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用QLORA訓練白話文和文言文互轉的模型(台大資工所2024年應用深度學習作業3)\n",
    "### 1. 作業目標\n",
    "- 本次作業目標是使用QLORA訓練一個白話文和文言文互轉的模型。\n",
    "- 使用的基礎模型是`zake7749/gemma-2-2b-it-chinese-kyara-dpo`，這是一個Google Gemma2經過Instrunction Tuned及DPO之後的模型。\n",
    "- 本次作業的資料集是台大資工作業提供的，並沒有經過資料清理，是用簡體硬轉成繁體的，資料集問題很多，這是一個包含白話文和文言文的資料集。\n",
    "### 2. 作業步驟\n",
    "- 本次作業的步驟如下：\n",
    "  1. 資料前處理\n",
    "  2. 使用QLORA訓練模型\n",
    "  3. 模型測試\n",
    "### 3. 訓練監控\n",
    "- 本次作業的訓練監控如下：\n",
    "  1. 訓練過程中的loss\n",
    "  2. 使用wandb記錄訓練過程\n",
    "  3. 調整各種參數觀察訓練結果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 首先撰寫一個在訓練時可以觀察記憶體及GPU使用的函數\n",
    "- 使用nvidia的`nvidia-ml-py3`套件來取得GPU的使用情況\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pynvml import *\n",
    "\n",
    "\n",
    "def print_gpu_utilization():\n",
    "    nvmlInit()\n",
    "    handle = nvmlDeviceGetHandleByIndex(0)\n",
    "    info = nvmlDeviceGetMemoryInfo(handle)\n",
    "    print(f\"GPU memory occupied: {info.used//1024**2} MB.\")\n",
    "\n",
    "\n",
    "def print_summary(result):\n",
    "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
    "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
    "    print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 印出GPU記憶體的使用量\n",
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"zake7749/gemma-2-2b-it-chinese-kyara-dpo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "no_quant_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "print(no_quant_model)\n",
    "no_quant_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "del no_quant_model\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = \"zake7749/gemma-2-2b-it-chinese-kyara-dpo\"\n",
    "\n",
    "\n",
    "# del no_quant_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             # device_map=\"auto\",\n",
    "                                             attn_implementation='eager',\n",
    "                                             cache_implementation=None,\n",
    "                                             use_cache=False,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_gpu_utilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化後的模型大小\n",
    "- 原來量化前後的模型參數量應該是完全一樣的，但這邊看到變成原來的1/4\n",
    "- 這是因為BitsAndBytes的實作方式，會將多個4位元的參數合併成一個32位元的參數，所以看起來會變小\n",
    "- 參考：https://discuss.huggingface.co/t/less-trainable-parameters-after-quantization/43192/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 印出可訓練參數數量\n",
    "trainable_params = count_trainable_params(model)\n",
    "print(f\"原來可訓練參數數量: {count_trainable_params(no_quant_model)}\")\n",
    "print(f\"量化後可訓練參數數量: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del no_quant_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"arbml/CIDAR\", split=\"train\")\n",
    "dataset = load_dataset('json', data_files=\"train.json\", split=\"train\").shuffle(seed=42)\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    prefix_text = '你是一個使用繁體中文的人工智慧助理，下面是問題的描述，以及對應的答案，請照著問題並且回答答案。\\n\\n'\n",
    "    text = f\"<start_of_turn>user {prefix_text} {data_point['instruction']} <end_of_turn>\\n<start_of_turn>model {data_point['output']} <end_of_turn>\"\n",
    "    return text\n",
    "# Add the 'prompt' column to the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", text_column)\n",
    "# Tokenize the dataset\n",
    "dataset = dataset.shuffle(seed=1234)\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "# Split the dataset into training and testing\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            if 'output_layer' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "modules = find_all_linear_names(model)  # Get modules to apply LoRA to\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(peft_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "# training_arguments = SFTConfig(dataset_text_field=\"prompt\", \n",
    "#                                output_dir=\"output1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=lora_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        output_dir=\"output1\",\n",
    "        # dataset_text_field=\"prompt\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_strategy=\"steps\",\n",
    "        # report_to=None,\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=1,\n",
    "        packing=False,\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"peft_model\")\n",
    "repo_name = \"huchiahsi/peft-model-repo\"  # 替換為你的 repository 名稱\n",
    "save_directory = \"./peft_model\"             # 模型儲存的本地路徑\n",
    "\n",
    "# 上傳模型到 Hugging Face\n",
    "peft_model.push_to_hub(repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "merged_model = PeftModel.from_pretrained(base_model, \"output2/checkpoint-100\")\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "# Save the merged model\n",
    "# merged_model.save_pretrained(\"merged_model\", safe_serialization=True, push_to_hub=True)\n",
    "# tokenizer.save_pretrained(\"merged_model\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import pipeline\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"huchiahsi/merged_model\", device=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
