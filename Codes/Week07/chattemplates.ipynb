{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = \"zake7749/gemma-2-2b-it-chinese-kyara-dpo\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             attn_implementation='eager',\n",
    "                                             cache_implementation=None,\n",
    "                                             use_cache=False,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "c2m_prompt = \"你是一個繁體中文的專家，專門將文言文翻譯成白話文。將下面的文言文翻譯成白話文：\"\n",
    "m2c_prompt = \"你是一個繁體中文的專家，專門將白話文翻譯成文言文。將下面的白話文翻譯成文言文：\"\n",
    "\n",
    "def c2m_get_prompt(example):\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": f\"{c2m_prompt}：{example['classical']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{example['modern']}\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "def m2c_get_prompt(example):\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": f\"{m2c_prompt}：{example['modern']}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{example['classical']}\"},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "c2m_dataset = load_dataset('csv', data_files=\"datasetcht.csv\", split=\"train\").shuffle(seed=42)\n",
    "c2m_text = [c2m_get_prompt(data_point) for data_point in c2m_dataset]\n",
    "c2m_dataset = c2m_dataset.add_column(\"prompt\", c2m_text)\n",
    "# c2m_dataset = c2m_dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "\n",
    "m2c_dataset = load_dataset('csv', data_files=\"datasetcht.csv\", split=\"train\").shuffle(seed=42)\n",
    "m2c_text = [m2c_get_prompt(data_point) for data_point in m2c_dataset]\n",
    "m2c_dataset = m2c_dataset.add_column(\"prompt\", m2c_text)\n",
    "# m2c_dataset = m2c_dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "dataset = concatenate_datasets([c2m_dataset, m2c_dataset])\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            if 'output_layer' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "modules = find_all_linear_names(model)  # Get modules to apply LoRA to\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=lora_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=10,\n",
    "        # max_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        output_dir=\"output1\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_strategy=\"steps\",\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=1,\n",
    "        packing=False,\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"peft_model\")\n",
    "repo_name = \"huchiahsi/peft-model-repo-1900k\"  # 替換為你的 repository 名稱\n",
    "save_directory = \"./peft_model\"             # 模型儲存的本地路徑\n",
    "\n",
    "# 上傳模型到 Hugging Face\n",
    "peft_model.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mustllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
