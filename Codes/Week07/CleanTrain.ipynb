{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_id = \"zake7749/gemma-2-2b-it-chinese-kyara-dpo\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             quantization_config=bnb_config, \n",
    "                                             attn_implementation='eager',\n",
    "                                             cache_implementation=None,\n",
    "                                             use_cache=False,)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)\n",
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "# dataset = load_dataset(\"arbml/CIDAR\", split=\"train\")\n",
    "dataset = load_dataset('json', data_files=\"train.json\", split=\"train\").shuffle(seed=42)\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    prefix_text = '你是一個使用繁體中文的人工智慧助理，下面是問題的描述，以及對應的答案，請照著問題並且回答答案。\\n\\n'\n",
    "    text = f\"<start_of_turn>user {prefix_text} {data_point['instruction']} <end_of_turn>\\n<start_of_turn>model {data_point['output']} <end_of_turn>\"\n",
    "    return text\n",
    "# Add the 'prompt' column to the dataset\n",
    "text_column = [generate_prompt(data_point) for data_point in dataset]\n",
    "dataset = dataset.add_column(\"prompt\", text_column)\n",
    "# Tokenize the dataset\n",
    "dataset = dataset.shuffle(seed=1234)\n",
    "dataset = dataset.map(lambda samples: tokenizer(samples[\"prompt\"]), batched=True)\n",
    "# Split the dataset into training and testing\n",
    "dataset = dataset.train_test_split(test_size=0.2)\n",
    "train_data = dataset[\"train\"]\n",
    "test_data = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_all_linear_names(peft_model, int4=False, int8=False):\n",
    "    \"\"\"Find all linear layer names in the model. reference from qlora paper.\"\"\"\n",
    "    cls = torch.nn.Linear\n",
    "    if int4 or int8:\n",
    "        import bitsandbytes as bnb\n",
    "        if int4:\n",
    "            cls = bnb.nn.Linear4bit\n",
    "        elif int8:\n",
    "            cls = bnb.nn.Linear8bitLt\n",
    "    lora_module_names = set()\n",
    "    for name, module in peft_model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            # last layer is not add to lora_module_names\n",
    "            if 'lm_head' in name:\n",
    "                continue\n",
    "            if 'output_layer' in name:\n",
    "                continue\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return sorted(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "modules = find_all_linear_names(model)  # Get modules to apply LoRA to\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "peft_model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTConfig\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=peft_model,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    peft_config=lora_config,\n",
    "    args=SFTConfig(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        output_dir=\"output1\",\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_strategy=\"steps\",\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=1,\n",
    "        packing=False,\n",
    "        gradient_checkpointing=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_model.save_pretrained(\"peft_model\")\n",
    "repo_name = \"huchiahsi/peft-model-repo\"  # 替換為你的 repository 名稱\n",
    "save_directory = \"./peft_model\"             # 模型儲存的本地路徑\n",
    "\n",
    "# 上傳模型到 Hugging Face\n",
    "peft_model.push_to_hub(repo_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
